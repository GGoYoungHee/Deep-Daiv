---
layout: post
title: "Notionìœ¼ë¡œ ê¸€ ì‘ì„±í•˜ê³  Github ioë¡œ ê¸€ ì˜®ê¸°ê¸°"
background: 
---

# GoogleNet Summary (Korean)

# 0. ë…¼ë¬¸ ì„ ì • ì´ìœ 

CNNì— ëŒ€í•´ ì¢€ ë” ê¹Šê²Œ ê³µë¶€í•´ë³´ê³  ì‹¶ì–´ì„œ GoogleNetì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤.

# 1. **Abstract**

ë¨¼ì € ì´ˆë¡ì—ì„œëŠ” GoogLeNetì˜ íŠ¹ì§•ì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…í•˜ê³  ìˆë‹¤.

ì´ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ì€Â **ì—°ì‚°ì„ í•˜ëŠ” ë° ì†Œëª¨ë˜ëŠ” ìì›ì˜ ì‚¬ìš© íš¨ìœ¨ì´ ê°œì„ **ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì •êµí•œ ì„¤ê³„ ë•ì— ë„¤íŠ¸ì›Œí¬ì˜ depthì™€ widthë¥¼ ëŠ˜ë ¤ë„ ì—°ì‚°ëŸ‰ì´ ì¦ê°€í•˜ì§€ ì•Šê³  ìœ ì§€ëœë‹¤ëŠ” ëœ»ì´ë‹¤. ì´ë•Œ, Google íŒ€ì—ì„œëŠ” ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´Â **Hebbian principleê³¼ multi-scale processingì„ ì ìš©**í•˜ì˜€ê³ , ì´ êµ¬ì¡°ë¥¼Â **GoogLeNet**ì´ë¼ ë¶€ë¥¸ë‹¤ê³  í•œë‹¤. GoogLeNetì€ 22ê°œì˜ layerë¥¼ ê°€ì§€ë©°, ì½”ë“œë„¤ì„ì€Â **Inception**ì´ë‹¤.

- Hebbian principleì´ë€?
    
    ì‹œëƒ…ìŠ¤ ì „ ì„¸í¬ì™€ í›„ ì„¸í¬ì˜ ë°˜ë³µì ì´ê³  ì§€ì†ì ì¸ ìê·¹ì—ì„œ ì‹œëƒ…ìŠ¤ íš¨ìš©ì˜ ì¦ê°€ê°€ ë°œìƒ
    ì‰½ê²Œ ì„¤ëª…í•˜ìë©´, ê°™ì€ ì¼ì„ ë°˜ë³µí–ˆì„ ë•Œ ë” ì‰½ê²Œ ê·¸ ì¼ì„ í•  ìˆ˜ ìˆê²Œë˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
    
    â‡’ ì¸ê³µì§€ëŠ¥ì—ì„œ ì„¤ëª…í•˜ìë©´, ì‹œëƒ…ìŠ¤(ê°€ì¤‘ì¹˜, Weight)ì™€ ì„¸í¬ì²´(í™œì„±í™”í•¨ìˆ˜, Activation function)ì˜ ê°œë…ì„ ê°–ëŠ” ì¸ê³µë‰´ëŸ°ì„ ë§Œë“¤ë©´ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì›ì¹™ì´ë‹¤.
    ì°¸ê³ ë§í¬) [https://en.wikipedia.org/wiki/Hebbian_theory](https://en.wikipedia.org/wiki/Hebbian_theory)
    

# **2. Introduction**

  ì§€ë‚œ 3ë…„ê°„, CNN ë¶„ì•¼ì— í° ë°œì „ì´ ì´ë£¨ì–´ì ¸ ì™”ëŠ”ë°, ì´ëŸ¬í•œ ë°œì „ì€ ë‹¨ì§€ ë” ì¢‹ì€ í•˜ë“œì›¨ì–´ì˜ ì„±ëŠ¥, ë” í° dataset, ë” í° ëª¨ë¸ ë•Œë¬¸ì´ê¸°ë³´ë‹¤ëŠ”Â **ìƒˆë¡œìš´ ì•„ì´ë””ì–´ì™€ ì•Œê³ ë¦¬ì¦˜, ê·¸ë¦¬ê³  ê°œì„ ëœ ì‹ ê²½ë§ êµ¬ì¡°**Â ë•ë¶„ì´ì—ˆë‹¤.

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn.png)

   GoogLeNetì€ AlexNetë³´ë‹¤ íŒŒë¼ë¯¸í„°ê°€ 12ë°°ë‚˜ ë” ì ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  í›¨ì”¬ ì •í™•í–ˆë‹¤ê³  í•œë‹¤. ì´ëŸ¬í•œ ê°œì„ ì€ ë§ˆì¹˜ R-CNNì²˜ëŸ¼Â **deepí•œ êµ¬ì¡°ì™€ í´ë˜ì‹í•œ ì»´í“¨í„° ë¹„ì „ì˜ ì‹œë„ˆì§€ ë•ë¶„ì´ì—ˆë‹¤.**

- R-CNNì´ë€ ?
    
    R-CNNì€ Image classificationì„ ìˆ˜í–‰í•˜ëŠ” CNNê³¼ localizationì„ ìœ„í•œ regional proposalì•Œê³ ë¦¬ì¦˜ì„ ì—°ê²°í•œ ëª¨ë¸ì´ë‹¤.
    ì°¸ê³ ) [https://ganghee-lee.tistory.com/35](https://ganghee-lee.tistory.com/35)
    
    ![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%201.png)
    

  Mobile ë° Embedded í™˜ê²½ì—ì„œëŠ” íŠ¹íˆ ì „ë ¥ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ì ì—ì„œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì¤‘ìš”ì„±ì´ ëŒ€ë‘ë˜ê³  ìˆê¸°ì—, ì´ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë¸ì´Â **ì—„ê²©í•œ ê³ ì •ëœ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê²ƒë³´ë‹¤ ìœ ì—°í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œë”**Â í•˜ì˜€ë‹¤. ë˜í•œÂ **ì¶”ë¡  ì‹œê°„ì— 1.5 billion ì´í•˜ì˜ ì—°ì‚°ë§Œì„ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„**í•˜ì—¬, ë‹¨ìˆœíˆ í•™ìˆ ì ì¸ í˜¸ê¸°ì‹¬ì— ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í˜„ì‹¤ì—ì„œë„ ì ì ˆíˆ ì‚¬ìš©ë˜ê²Œë” ì„¤ê³„í•˜ì˜€ë‹¤.

# 3. Related Work

GoogLeNetì˜ ì½”ë“œë„¤ì„ì¸ Inceptionì´ë€ ì´ë¦„ì€Â **Network in Network(NIN)ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ìœ ë˜í•˜ì˜€ìœ¼ë©°,**Â ë” ì •í™•í•˜ê²ŒëŠ” ì¸ì…‰ì…˜ ì˜í™”ì˜ ëŒ€ì‚¬ì¸ "we need to go deeper"ì—ì„œ ì°©ì•ˆí•˜ì˜€ë‹¤. ì´ë•Œ "deep"ì€ ë‘ ê°€ì§€ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.

1. "Inception module"ì˜ í˜•íƒœë¡œ ìƒˆë¡œìš´ ì°¨ì›ì˜ êµ¬ì¡° ë„ì…

2. ë‘ ë²ˆì§¸ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´ê°€ ì¦ê°€í•˜ì˜€ë‹¤ëŠ” ì§ì ‘ì ì¸ ì˜ë¯¸

LeNet-5ë¥¼ ì‹œì‘ìœ¼ë¡œÂ **CNNì€ ì¼ë°˜ì ì¸ í‘œì¤€ êµ¬ì¡°**ë¥¼ ê°€ì§€ê²Œ ë˜ëŠ”ë°, ì´ëŠ”Â **Convolutional layerê°€ ìŒ“ì´ê³  ê·¸ ë’¤ì— 1ê°œ ë˜ëŠ” ê·¸ ì´ìƒì˜ FC layerê°€ ë”°ë¼ì˜¤ëŠ” êµ¬ì¡°**ì´ë‹¤. ë˜í•œ ImageNetê³¼ ê°™ì´ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œì˜ ìš”ì¦˜ íŠ¸ë ŒíŠ¸ëŠ”Â **layerì˜ ìˆ˜ì™€ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ê³ , ì˜¤ë²„ í”¼íŒ…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ dropoutì„ ì ìš©**í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ GoogLeNetë„ ì´ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ë¤ë‹¤.

ë‹¤ìŒìœ¼ë¡œ Network in Network ë…¼ë¬¸ ë‚´ìš©ì´ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë°, ì´ëŠ” GoogLeNetì— ë§ì€ ì˜í–¥ì„ ë¼ì¹œ ë…¼ë¬¸ì´ë‹¤.

ë¨¼ì € Network in NetworkëŠ” ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì œì•ˆëœ ì ‘ê·¼ë²•ì´ë‹¤. ì´ ë°©ë²•ì€Â **1 x 1 Convolutional layerê°€ ì¶”ê°€ë˜ë©°, ReLU activationì´ ë’¤ë”°ë¥¸ë‹¤.**Â 

 ë” ê¹Šê²Œ ì•Œê³ ì‹¶ë‹¤ë©´...

- NIN(Network in Network)ê³¼ ê·¸ ì´ì „ ëª¨ë¸ì¸ ZFNetì— ëŒ€í•´ ì•Œì•„ë³´ì
    1. ZFNet
    [https://m.blog.naver.com/laonple/222488895057](https://m.blog.naver.com/laonple/222488895057)
    ê·¸ê°„ CNNì€ ë¸”ë™ë°•ìŠ¤ ëª¨ë¸, ì¦‰ ë‚´ë¶€ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ëª°ëë‹¤. í•˜ì§€ë§Œ ZFNetì€ Visualization(ì‹œê°í™”)ë¥¼ í†µí•´ ë‚´ë¶€ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ” ì§€ ì•Œê²Œ ë˜ì—ˆê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì¢‹ì€ êµ¬ì¡°ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆê²Œ ë˜ì–´ í–¥í›„ CNN ë°œì „ì— í† ëŒ€ê°€ ë¨.
    2. NIN
    Network in Networkì—ì„œëŠ”Â **ë¹„ì„ í˜•ì  ê´€ê³„ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ ë‹¨ìˆœí•œ Convolution ì—°ì‚°ì´ ì•„ë‹Œ, Multi Layer Perception ì¦‰, MLPë¥¼ ì¤‘ê°„ì— ë„£ê²Œ ëœë‹¤.**
        
         [https://m.blog.naver.com/laonple/222504298493](https://m.blog.naver.com/laonple/222504298493)
        
        - MLP Convolutional Layer
            
            ![ê·¸ë¦¼_1._Convì™€_Mlpconv_layerì˜_êµ¬ì¡°_ë¹„êµ.jpg](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/%EA%B7%B8%EB%A6%BC_1._Conv%EC%99%80_Mlpconv_layer%EC%9D%98_%EA%B5%AC%EC%A1%B0_%EB%B9%84%EA%B5%90.jpg)
            
            NIN ì—°êµ¬ì§„ë“¤ì€, ì¼ë°˜ì ì¸ CNN êµ¬ì¡°ì˜ convolutional layerê°€ local receptive fieldì—ì„œ ì–´ë–¤ íŠ¹ì§•ì„ ì¶”ì¶œí•´ë‚´ëŠ” ëŠ¥ë ¥ì€ ìš°ìˆ˜í•˜ì§€ë§Œ, ì—¬ê¸°ì— ì‚¬ìš©í•˜ëŠ” í••í„°ê°€ ì„ í˜•ì (linear)ì´ê¸° ë•Œë¬¸ì—, ë¹„ì„ í˜•ì (non-linear)ì¸ íŠ¹ì§•ì„ ì¶”ì¶œí•´ë‚´ëŠ” ë°ëŠ” ì–´ë ¤ì›€ì´ ìˆìœ¼ë©°, ì´ ë¶€ë¶„ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ feature-mapì˜ ìˆ˜ë¥¼ ëŠ˜ë ¤ì•¼ í•œë‹¤ëŠ” ì ì— ì£¼ëª©í–ˆë‹¤. í•„í„°ì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ì—°ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
            
            ê·¸ë˜ì„œ NIN ì„¤ê³„ì§„ì€ local receptive field ì•ˆì—ì„œ íŠ¹ì§•ì„ ë” ì˜ ì¶”ì¶œí•´ ë‚¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ì˜€ìœ¼ë©°, ê²°ê³¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ MLP Convolutional Layerì´ë©°, ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.
            
        - CCCP(Cascaded Cross Channel Pooling) ê¸°ë²• (ë…¼ë¬¸ì— ì ìš©ëœ 1x1 conv layerì™€ ìœ ì‚¬)
            
            Network in Networkì—ì„œ ì´ìš©í•˜ì˜€ë˜Â **CCCP**Â (Cascaded Cross Channel Pooling)ë¼ëŠ” ê¸°ë²•ì´ ìˆë‹¤. ì´ëŠ” í•˜ë‚˜ì˜ feature mapì— ëŒ€í•˜ì—¬ ìˆ˜í–‰í•˜ëŠ” ì¼ë°˜ì ì¸ pooling ê¸°ë²•ê³¼ëŠ” ë‹¬ë¦¬Â **channelì„ ì§ë ¬ë¡œ ë¬¶ì–´ í”½ì…€ ë³„ë¡œ poolingì„ ìˆ˜í–‰**í•˜ëŠ” ê²ƒì¸ë°, ì´ëŸ¬í•œ CCCP ì—°ì‚°ì˜ íŠ¹ì§•ì€Â **feature mapì˜ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œì´ê³ , channelì˜ ìˆ˜ë§Œ ì¤„ì–´ë“¤ê²Œ í•˜ì—¬ ì°¨ì› ì¶•ì†Œì˜ íš¨ê³¼**ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
            
            ![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%202.png)
            
            ê·¸ëŸ°ë° ì´Â **CCCP ê¸°ë²•ì€ 1 x 1 Convolutional layerê³¼ ê·¸ ì—°ì‚° ë°©ì‹ ë° íš¨ê³¼ê°€ ë§¤ìš° ìœ ì‚¬**í•˜ë‹¤. ë”°ë¼ì„œ GoogLeNetì—ì„œ 1 x 1 Convolutional layerë¥¼ Inception moduleì— ì ìš©í•œ ê²ƒì´ë‹¤.
            
            - 1x1ì˜ ì¥ì 
                
                ì°¸ê³ ë§í¬) [https://hwiyong.tistory.com/45](https://hwiyong.tistory.com/45)
                
                ì¥ì  3ê°€ì§€
                
                1. Channel ìˆ˜ ì¡°ì ˆ
                2. ì—°ì‚°ëŸ‰ ê°ì†Œ(Efficient)
                3. ë¹„ì„ í˜•ì„±(Non-linearity)
                    
                    ![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%203.png)
                    
        
    

# **4. Motivation and High Level Considerations**

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%204.png)

GoogLeNetì´ ë‚˜ì˜¤ê²Œ ëœ ë°°ê²½ì— ëŒ€í•´ì„œ ì„¤ëª…í•œë‹¤.

**ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ì„±ëŠ¥ì„ ê°œì„ ì‹œí‚¬ ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì‹ ê²½ë§ì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒ**ì´ë‹¤. ì´ë•Œ, í¬ê¸°ë¥¼ ëŠ˜ë¦°ë‹¤ëŠ” ê²ƒì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì˜ë¯¸ë¥¼ ëœ»í•œë‹¤.

1.Â **depthì˜ ì¦ê°€**Â (levelì˜ ìˆ˜ ì¦ê°€)

2.Â **widthì˜ ì¦ê°€**Â (ê° levelì˜ ìœ ë‹› ìˆ˜ ì¦ê°€)

ì´ëŠ” ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ì‰½ê³  ì•ˆì „í•œ ë°©ë²•ì´ì§€ë§Œ, ë‘ ê°€ì§€ ë¬¸ì œì ì´ ìˆë‹¤.

**ì²« ë²ˆì§¸ë¡œ, í¬ê¸°ê°€ ì»¤ì§„ë‹¤ëŠ” ê²ƒì€ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚œë‹¤ëŠ” ê²ƒì¸ë°, ì´ëŠ” íŠ¹íˆ í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì— ì˜¤ë²„ í”¼íŒ…ì´ ì¼ì–´ë‚˜ê¸° ì‰½ë‹¤.**

ì´ëŠ”Â **ì£¼ìš”í•œ ë³‘ëª©í˜„ìƒ**ì´ ë  ìˆ˜ë„ ìˆëŠ”ë° ImageNetì²˜ëŸ¼ ì„¸ë°€í•œ ì¹´í…Œê³ ë¦¬ë¥¼ êµ¬ë³„í•´ì•¼ í•˜ëŠ” ê²½ìš°,Â **ê³ í’ˆì§ˆì˜ íŠ¸ë ˆì´ë‹ ì…‹ì„ ìƒì„±í•˜ëŠ” ê²ƒì€ ë§¤ìš° tricky í•˜ë©° ë¹„ìš©ì´ ë†’ê¸° ë•Œë¬¸**ì´ë‹¤.

**ë‘ ë²ˆì§¸ë¡œ, ë„¤íŠ¸ì›Œí¬ê°€ ì»¤ì§ˆìˆ˜ë¡ ì»´í“¨í„° ìì›ì˜ ì‚¬ìš©ëŸ‰ì´ ëŠ˜ì–´ë‚œë‹¤ëŠ” ê²ƒì´ë‹¤.**Â ë§Œì•½ ë‘ Convolutional layerê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´,Â **í•„í„°ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚  ë•Œ ì—°ì‚°ëŸ‰ì„ quadratic í•˜ê²Œ ì¦ê°€**ì‹œí‚¬ ê²ƒì´ë‹¤.

ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ì»´í“¨íŒ… ìì›ì€ í•œì •ì ì´ë¯€ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤ ì»´í“¨íŒ… ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°í•˜ëŠ” ê²ƒì´ ë”ìš± ì¤‘ìš”í•˜ë‹¤.

![ì™¼ìª½ì€ Sparseí•œ ë„¤íŠ¸ì›Œí¬ì´ê³ , ì˜¤ë¥¸ìª½ì€ Denseí•œ ë„¤íŠ¸ì›Œí¬ì´ë‹¤](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%205.png)

ì™¼ìª½ì€ Sparseí•œ ë„¤íŠ¸ì›Œí¬ì´ê³ , ì˜¤ë¥¸ìª½ì€ Denseí•œ ë„¤íŠ¸ì›Œí¬ì´ë‹¤

ìœ„ì˜ ë‘ê°€ì§€ ë¬¸ì œë¥¼ ëª¨ë‘ í•´ê²°í•˜ê¸° ìœ„í•œ ê·¼ë³¸ì ì¸ ë°©ë²•ì€ convolutional layerë‚´ë¶€ì˜ fully connectedë¥¼ sparsely connected êµ¬ì¡°ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ê°€ì¥ ì´ìƒì ì¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ëŠ” ì¶œë ¥ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  highly correlatedëœ ì¶œë ¥ë¼ë¦¬ ëª¨ì€ë‹¤ìŒ ì¸µë³„ë¡œ ì„¤ê³„í•˜ëŠ” ê²ƒì´ë‹¤.

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%206.png)

í•˜ì§€ë§Œ ì»´í“¨í„°ëŠ” ì‚¬ê°ì´ ì•„ë‹Œ sparse matrixê³„ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì§€ ëª»í•œë‹¤. vision ë¶„ì•¼ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œë“¤ì€ convolutuionì„ ì´ìš©í•´ì„œ spatial domainì—ì„œëŠ” sparsityë¥¼ ì´ìš©í•˜ëŠ”ë“¯ í•˜ì§€ë§Œ, convolutionìì²´ê°€ dense connectionsì˜ ëª¨ìŒìœ¼ë¡œ êµ¬í˜„ëœë‹¤.

> ë„ˆë¬´ ì–´ë ¤ì›Œì„œ,,, ë¶€ì—°ì„¤ëª…,, ë„ ì–´ë µìŠµë‹ˆë‹¤
> 
> 
> convolutionì€ ì‹¤ì œ ì‹œì‹ ê²½ì—ì„œ íŠ¹ì • íŒ¨í„´ì— íŠ¹ì •ë‰´ëŸ°ì´ ë°˜ì‘í•˜ëŠ” ê²ƒì„ ë³¸ë”°ì„œ ë§Œë“¤ì—ˆë‹¤. ì¦‰ í•„í„°ëŠ” íŠ¹ì • ë‰´ëŸ°ì´ê³ , ì…ë ¥ ì´ë¯¸ì§€ì˜ íŒ¨í„´ì— ì¼ë¶€ í•„í„°ë§Œ ë°˜ì‘í•˜ë¯€ë¡œ(ê°’ì´ í¬ë¯€ë¡œ) sparsityë¥¼ ì´ìš©í•œë‹¤ê³  ë§ í–ˆê±°ë‚˜, convolutionìì²´ê°€ patchì˜ í•œ í”½ì…€ë‹¹ ê° í•„í„° ì „ì²´ê°€ ì•„ë‹Œ í•œ í”½ì…€ê³¼ë§Œ ê³±í•´ì§€ë¯€ë¡œ sparsityí•˜ë‹¤ê³  ë§í•œ ê²ƒ ê°™ë‹¤.
> 
> dense connection ë¶€ë¶„ì€ ì‹¤ì œêµ¬í˜„ì—ì„œ conv layerì˜ ì…ë ¥ì€ 3ì°¨ì›ì´ë‚˜ 4ì°¨ì›ì´ë¼ ê³„ì‚°ì´ ë¹„íš¨ìœ¨ì ì´ë‹¤. ë”°ë¼ì„œ, íš¨ìœ¨ì„±ì„ ìœ„í•´ ì´ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜í›„ í–‰ë ¬ ë‚´ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ”ë°, í–‰ë ¬ ë‚´ì ì€ dense connectionì´ë¯€ë¡œ convolutuionì€ dense connectionìœ¼ë¡œ êµ¬í˜„ëœë‹¤ê³  ë§í•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
> 

ConvNetì€ LeNetì´í›„ë¡œ symmetryë¥¼ ê¹¨ê³  í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œ feature ì°¨ì›ì—ì„œì˜ random í˜¹ì€ sparse connection tableì„ ì‚¬ìš© í–ˆë‹¤. í•˜ì§€ë§Œ, ë³‘ë ¬ ê³„ì‚°ì„ ë” ìµœì í™”í•˜ê¸° ìœ„í•´ AlexNetì˜ full connectionìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤.

Inception êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ filter-levelê³¼ ê°™ì€ ë‹¨ê³„ì—ì„œ sparsely connected êµ¬ì¡°ë¥¼ ë§Œë“¤ê¹Œì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤.

# **5. Architectural Details**

ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ Inception êµ¬ì¡°ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•œë‹¤.

**Inception êµ¬ì¡°ì˜ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” CNNì—ì„œ ê° ìš”ì†Œë¥¼ ìµœì ì˜ local sparce structureë¡œ ê·¼ì‚¬í™”í•˜ê³ , ì´ë¥¼ dense componentë¡œ ë°”ê¾¸ëŠ” ë°©ë²•ì„ ì°¾ëŠ” ê²ƒ**ì´ë‹¤. ì¦‰, ìµœì ì˜ local êµ¬ì„± ìš”ì†Œë¥¼ ì°¾ê³  ì´ë¥¼ ê³µê°„ì ìœ¼ë¡œ ë°˜ë³µí•˜ë©´ ëœë‹¤. ì´ë¥¼ ì‰½ê²Œ ë§í•˜ìë©´Â **Sparse ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì„œë¡œ ë¬¶ì–´ (í´ëŸ¬ìŠ¤í„°ë§ í•˜ì—¬) ìƒëŒ€ì ìœ¼ë¡œ Dense í•œ Submatrixë¥¼ ë§Œë“ ë‹¤ëŠ” ê²ƒ**ì´ë‹¤.

ì´ë•Œ, ì´ì „ layerì˜ ê° ìœ ë‹›ì´ ì…ë ¥ ì´ë¯¸ì§€ì˜ íŠ¹ì • ë¶€ë¶„ì— í•´ë‹¹ëœë‹¤ê³  ê°€ì •í•˜ì˜€ëŠ”ë°, ì…ë ¥ ì´ë¯¸ì§€ì™€ ê°€ê¹Œìš´ ë‚®ì€ layerì—ì„œëŠ” íŠ¹ì • ë¶€ë¶„ì— Correlated unitë“¤ì´ ì§‘ì¤‘ë˜ì–´ ìˆë‹¤. ì´ëŠ” ë‹¨ì¼ ì§€ì—­ì— ë§ì€ í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì§‘ì¤‘ëœë‹¤ëŠ” ëœ»ì´ê¸°ì— 1 x 1 Convolutionìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

![ë§¨ ì˜¤ë¥¸ìª½ì˜ ì›í˜• ëª¨ì–‘ì€ í•„í„°ê°€ ë” ì»¤ì•¼ ì—°ê´€ëœ ìœ ë‹›ì„ ë”ìš± ë§ì´ ë½‘ì•„ë‚¼ ìˆ˜ ìˆë‹¤.](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%207.png)

ë§¨ ì˜¤ë¥¸ìª½ì˜ ì›í˜• ëª¨ì–‘ì€ í•„í„°ê°€ ë” ì»¤ì•¼ ì—°ê´€ëœ ìœ ë‹›ì„ ë”ìš± ë§ì´ ë½‘ì•„ë‚¼ ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ, ëª‡ëª‡ ìœ„ì¹˜ì—ì„œëŠ” ìœ„ ê·¸ë¦¼ì—ì„œì²˜ëŸ¼Â **ì¢€ ë” ë„“ì€ ì˜ì—­ì˜ Convolutional filterê°€ ìˆì–´ì•¼ Correlated unitì˜ ë¹„ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ìƒí™©**ì´ ë‚˜íƒ€ë‚  ìˆ˜ë„ ìˆë‹¤. ë”°ë¼ì„œÂ **feature mapì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ 1 x 1, 3 x 3, 5 x 5 convolution ì—°ì‚°ì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰**í•œë‹¤.

ë˜í•œ CNNì—ì„œ pooling layerì˜ ì„±ëŠ¥ì€ ì´ë¯¸ ì…ì¦ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ì™€ í•¨ê»˜ ë†’ì´ì™€ í­ì„ ë§ì¶”ê¸° ìœ„í•´ paddingë„ ì¶”ê°€í•´ì¤€ë‹¤.

> paddingì´ë€? 
í¬ê¸°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ matrix ì™¸ê°ì„ ë¹™ ë‘˜ëŸ¬ì„œ 1~2 í”½ì…€ ì •ë„ë¥¼ ë” í¬ê²Œ ë§Œë“œëŠ” ê²ƒ 
[https://brunch.co.kr/@coolmindory/37](https://brunch.co.kr/@coolmindory/37)
> 

![img1.daumcdn.jpg](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn.jpg)

1 x 1, 3 x 3, 5 x 5 Convolutional filterì˜ ìˆ˜ëŠ” ë§ì´ ê¹Šì–´ì§ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ”ë°, ë§Œì•½ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë†’ì€ layerì—ì„œë§Œ í¬ì°©ë  ìˆ˜ ìˆëŠ” ë†’ì€ ì¶”ìƒì  ê°œë…ì˜ íŠ¹ì§•ì´ ìˆë‹¤ë©´, ê³µê°„ì  ì§‘ì¤‘ë„ê°€ ê°ì†Œí•˜ê²Œ ë˜ì–´, ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ì— ë”°ë¼ 3 x 3ê³¼ 5 x 5 Convolutional filterì˜ ìˆ˜ë„ ëŠ˜ì–´ë‚˜ì•¼ í•œë‹¤.

**ê·¸ëŸ°ë° ì—¬ê¸°ì„œ í° ë¬¸ì œê°€ ë°œìƒí•œë‹¤!**

3 x 3 Convolutional filter ë¿ë§Œ ì•„ë‹ˆë¼, 5 x 5 Convolutional filterë„ ì‚¬ìš©í•  ê²½ìš°, ì—°ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ”ë° ì…ë ¥ feature mapì˜ í¬ê¸°ê°€ í¬ê±°ë‚˜ 5 x 5 Convolutional filterì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´Â **ì—°ì‚°ëŸ‰ì€ ë”ìš± ì¦ê°€í•˜ê²Œ ëœë‹¤.**

![ì—°ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì™¼ìª½ ëª¨ë¸ì—ì„œ 1 x 1 Convolution layerë¥¼ ì¶”ê°€í•˜ì˜€ë‹¤](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%208.png)

ì—°ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì™¼ìª½ ëª¨ë¸ì—ì„œ 1 x 1 Convolution layerë¥¼ ì¶”ê°€í•˜ì˜€ë‹¤

ë”°ë¼ì„œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´Â **1 x 1 Convolutional filterë¥¼ ì´ìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œ**í•˜ì˜€ë‹¤.Â **3 x 3ê³¼ 5 x 5 ì•ì— 1 x 1ì„ ë‘ì–´ ì°¨ì›ì„ ì¤„ì´ëŠ”ë°, ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ Scaleì„ í™•ë³´í•˜ë©´ì„œë„ ì—°ì‚°ëŸ‰ì„ ë‚®ì¶œ ìˆ˜ ìˆë‹¤.**

ì¶”ê°€ì ìœ¼ë¡œ,Â **Convlution ì—°ì‚° ì´í›„ì— ì¶”ê°€ë˜ëŠ” ReLUë¥¼ í†µí•´ ë¹„ì„ í˜•ì  íŠ¹ì§•ì„ ë” ì¶”ê°€**í•  ìˆ˜ ìˆë‹¤.

ë˜í•œ, Google íŒ€ì—ì„œëŠ”Â **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ„í•´ ë‚®ì€ layerì—ì„œëŠ” ê¸°ë³¸ì ì¸ CNN ëª¨ë¸ì„ ì ìš©**í•˜ê³ ,Â **ë†’ì€ layerì—ì„œ Inception moduleì„ ì‚¬ìš©**í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  í•œë‹¤.

ì´ëŸ¬í•œ íŠ¹ì§•ë“¤ì„ ê°€ì§„ Inception moduleì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒ ë‘ ê°€ì§€ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

1.Â **ê³¼ë„í•œ ì—°ì‚°ëŸ‰ ë¬¸ì œì—†ì´ ê° ë‹¨ê³„ì—ì„œ ìœ ë‹› ìˆ˜ë¥¼ ìƒë‹¹íˆ ì¦ê°€**ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì´ëŠ” ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ë‹¤ìŒ layerì˜ input ìˆ˜ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

2.Â **Visual ì •ë³´ê°€ ë‹¤ì–‘í•œ Scaleë¡œ ì²˜ë¦¬ë˜ê³ , ë‹¤ìŒ layerëŠ” ë™ì‹œì— ì„œë¡œ ë‹¤ë¥¸ layerì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œ**í•  ìˆ˜ ìˆë‹¤. 1 x 1, 3 x 3, 5 x 5 Convolution ì—°ì‚°ì„ í†µí•´ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

# 6. GoogleNet

ì´ì œ Inception moduleì´ ì ìš©ëœ ì „ì²´ GoogLeNetì˜ êµ¬ì¡°ì— ëŒ€í•´ì„œ ì•Œì•„ë³¸ë‹¤.

![ê·¸ë¦¼5.jpg](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/%EA%B7%B8%EB%A6%BC5.jpg)

ë¨¼ì €, GoogLeNetì´ë¼ëŠ” ì´ë¦„ì€ LeNetìœ¼ë¡œë¶€í„° ìœ ë˜í•˜ì˜€ìœ¼ë©°, ì´ëŠ” Inception êµ¬ì¡°ì˜ ì„±ì²´ë¼ê³  í•œë‹¤.

**Inception module ë‚´ë¶€ë¥¼ í¬í•¨í•œ ëª¨ë“  Convolution layerì—ëŠ” ReLUê°€ ì ìš©**ë˜ì–´ ìˆë‹¤. ë˜í•œ receptive fieldì˜ í¬ê¸°ëŠ” 224 x 224ë¡œ RGB ì»¬ëŸ¬ ì±„ë„ì„ ê°€ì§€ë©°, mean subtractionì„ ì ìš©í•œë‹¤.

![GoogLeNetì˜ ì„¸ë¶€ êµ¬ì„±](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%209.png)

GoogLeNetì˜ ì„¸ë¶€ êµ¬ì„±

- í‘œ ì„¤ëª… ë§í¬) [https://m.blog.naver.com/PostView.naver?blogId=laonple&logNo=222518300329&referrerCode=0&searchKeyword=googlenet](https://m.blog.naver.com/PostView.naver?blogId=laonple&logNo=222518300329&referrerCode=0&searchKeyword=googlenet)
    - Patch size/stride:Â ì»¤ë„ì˜Â í¬ê¸°ì™€Â strideÂ ê°„ê²©.Â ì˜ˆë¥¼Â ë“¤ì–´,Â ìµœì´ˆì˜Â convolutionì—Â ìˆëŠ”Â 7x7/2ì˜Â ì˜ë¯¸ëŠ”Â receptive fieldì˜Â í¬ê¸°ê°€Â 7x7ì¸Â filterë¥¼Â 2í”½ì…€Â ê°„ê²©(stride)ìœ¼ë¡œÂ ì ìš©í•œë‹¤ëŠ”Â ëœ»ì´ë‹¤.
    - Output size:Â ì–»ì–´ì§€ëŠ”Â feature-mapì˜Â í¬ê¸°Â ë°Â ê°œìˆ˜. 112x112x64ì˜Â ì˜ë¯¸ëŠ”Â 224x224Â í¬ê¸°ì˜Â ì´ë¯¸ì§€ì—Â 2í”½ì…€Â ê°„ê²©ìœ¼ë¡œÂ 7x7 filterë¥¼Â ì ìš©í•˜ì—¬Â ì´Â 64ê°œì˜Â feature-mapì´Â ì–»ì–´ì¡Œë‹¤ëŠ”Â ëœ»ì´ë‹¤.
    - Depth:Â ì—°ì†ìœ¼ë¡œÂ convolution layerì˜Â ê°œìˆ˜.Â ì²«ë²ˆì§¸Â convolution layerëŠ”Â depthê°€Â 1ì´ê³ ,Â ë‘ë²ˆì§¸ì™€Â ì¸ì…‰ì…˜ì´Â ì ìš©ë˜ì–´Â ìˆëŠ”Â ë¶€ë¶„ì€Â ëª¨ë‘Â 2ë¡œÂ ë˜ì–´Â ìˆëŠ”Â ì´ìœ ëŠ”Â 2ê°œì˜Â convolutionì„Â ì—°ì†ì ìœ¼ë¡œÂ ì ìš©í•˜ê¸°Â ë•Œë¬¸ì´ë‹¤.
    - #1x1: 1x1 convolutionì„Â ì˜ë¯¸.Â ê·¸Â í–‰ì—Â ìˆëŠ”Â ìˆ«ìëŠ”Â 1x1 convolutionì„Â ìˆ˜í–‰í•œÂ ë’¤Â ì–»ì–´ì§€ëŠ”Â feature-mapì˜Â ê°œìˆ˜ë¥¼Â ë§í•œë‹¤.Â ì²«ë²ˆì§¸Â ì¸ì…‰ì…˜Â 3(a)ì˜Â #1x1Â ìœ„ì¹˜ì—Â ìˆëŠ”Â ìˆ«ìê°€Â 64ì¸ë°Â ì´ê²ƒì€Â ì´ì „Â layerì˜Â 192ê°œÂ feature-mapì„Â ì…ë ¥ìœ¼ë¡œÂ ë°›ì•„Â 64ê°œì˜Â feature-mapì´Â ì–»ì–´ì¡Œë‹¤ëŠ”Â ëœ»ì´ë‹¤.Â ì¦‰, 192ì°¨ì›ì´Â 64ì°¨ì›ìœ¼ë¡œÂ ì¤„ì–´ë“¤ê²ŒÂ ëœë‹¤.
    - #3x3 reduce: 3x3 convolutionÂ ì•ìª½ì—Â ìˆëŠ”Â 1x1 convolutionÂ ì„Â ì˜ë¯¸.Â ì¸ì…‰ì…˜Â 3(a)ì˜Â ìˆ˜ë¥¼Â ë³´ë©´Â 96ì´Â ìˆëŠ”ë°,Â ì´ê²ƒì€Â 3x3 convolutionì„Â ìˆ˜í–‰í•˜ê¸°Â ì „ì—Â 192ì°¨ì›ì„Â 96ì°¨ì›ìœ¼ë¡œÂ ì¤„ì¸Â ê²ƒì„Â ì˜ë¯¸í•œë‹¤.
    - #3x3: 1x1 convolutionì—Â ì˜í•´Â ì°¨ì›ì´Â ì¤„ì–´ë“ Â feature mapì—Â ì—°ê²°ë˜ëŠ”Â 3x3 convolution.Â ì¸ì…‰ì…˜Â 3(a)ì˜Â ìˆ«ìÂ 128ì€Â ìµœì¢…ì ìœ¼ë¡œÂ 1x1 convolutionê³¼Â 3x3 convolutionì„Â ì—°ì†ìœ¼ë¡œÂ ì ìš©í•˜ì—¬Â 128ê°œì˜Â feature-mapì´Â ì–»ì–´ì¡Œë‹¤ëŠ”Â ëœ»ì´ë‹¤.
    - #5x5 reduce:Â í•´ì„Â ë°©ë²•ì€Â â€œ#3x3 reduceâ€ì™€Â ë™ì¼í•˜ë‹¤.
    - #5x5:Â í•´ì„Â ë°©ë²•ì€Â â€œ#3x3â€ê³¼Â ë™ì¼í•˜ë‹¤. #5x5ëŠ”Â ì¢€Â ë”Â ë„“ì€Â ì˜ì—­ì—Â ê±¸ì³Â ìˆëŠ”Â featureë¥¼Â ì¶”ì¶œí•˜ê¸°Â ìœ„í•œÂ ìš©ë„ë¡œÂ ì¸ì…‰ì…˜Â ëª¨ë“ˆì—Â ì ìš©ì´Â ë˜ì—ˆë‹¤.
    - Pool/proj: max-poolingê³¼Â max-poolingÂ ë’¤ì—Â ì˜¤ëŠ”Â 1x1 convolutionì„Â ì ìš©í•œÂ ê²ƒì„Â ì˜ë¯¸.Â ì¸ì…‰ì…˜Â 3(a)Â ì—´ì˜Â ìˆ«ìÂ 32Â ëŠ”Â max-poolingê³¼Â 1x1 convolutionì„Â ê±°ì³Â ì´Â 32ê°œì˜Â feature-mapì´Â ì–»ì–´ì¡Œë‹¤ëŠ”Â ëœ»ì´ë‹¤.
    - Params:Â í•´ë‹¹Â layerì—Â ìˆëŠ”Â free parameterì˜Â ê°œìˆ˜.Â ì…ì¶œë ¥Â feature-mapì˜Â æ•¸ì—Â ë¹„ë¡€í•œë‹¤.Â ì¸ì…‰ì…˜Â 3(a)Â ì—´ì—Â ìˆëŠ”Â ìˆ«ìÂ 159KëŠ”Â ì´Â 256ê°œì˜Â feature-mapì„Â ë§Œë“¤ê¸°Â ìœ„í•´Â 159Kì˜Â free-parameterê°€Â ì ìš©ë˜ì—ˆë‹¤ëŠ”Â ëœ»ì´ë‹¤.
    - Ops:Â ì—°ì‚°ì˜Â ìˆ˜.Â ì—°ì‚°ì˜Â ìˆ˜ëŠ”Â feature-mapì˜Â ìˆ˜ì™€Â ì…ì¶œë ¥Â feature-mapì˜Â í¬ê¸°ì—Â ë¹„ë¡€í•œë‹¤.Â ì¸ì…‰ì…˜Â 3(a)ì˜Â ë‹¨ê³„ì—ì„œëŠ”Â ì´Â 128Mì˜Â ì—°ì‚°ì„Â ìˆ˜í–‰í•œë‹¤.

GoogLeNetì„ 4ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‚´í´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

**Part 1**

![ë‚®ì€ ë ˆì´ì–´ê°€ ìœ„ì¹˜í•´ ìˆëŠ” ë¶€ë¶„](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%2010.png)

ë‚®ì€ ë ˆì´ì–´ê°€ ìœ„ì¹˜í•´ ìˆëŠ” ë¶€ë¶„

Part 1ì€Â **ì…ë ¥ ì´ë¯¸ì§€ì™€ ê°€ê¹Œìš´ ë‚®ì€ ë ˆì´ì–´ê°€ ìœ„ì¹˜í•´ ìˆëŠ” ë¶€ë¶„**ì´ë‹¤.

ì´ëŠ” ìœ„ì—ì„œ ì„¤ëª…í–ˆë“¯ì´Â **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ„í•´ ë‚®ì€ layerì—ì„œëŠ” ê¸°ë³¸ì ì¸ CNN ëª¨ë¸ì„ ì ìš©**í•˜ê³ , ë†’ì€ layerì—ì„œ Inception moduleì„ ì‚¬ìš©í•˜ë¼ê³  í•˜ì˜€ê¸°ì— Inception moduleì´ ì‚¬ìš©ë˜ì§€ ì•Šì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

**Part 2**

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%2011.png)

Part 2ëŠ”Â **Inception module**ë¡œì„œÂ **ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ 1 x 1, 3 x 3, 5 x 5 Convolutional layerê°€ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰**í•˜ê³  ìˆìœ¼ë©°,Â **ì°¨ì›ì„ ì¶•ì†Œí•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 1 x 1 Convolutional layerê°€ ì ìš©**ë˜ì–´ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

**Part 3**

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%2012.png)

Part 3ëŠ”Â **auxiliary classifierê°€ ì ìš©ëœ ë¶€ë¶„**ì´ë‹¤.

ëª¨ë¸ì˜Â **ê¹Šì´ê°€ ë§¤ìš° ê¹Šì„ ê²½ìš°, ê¸°ìš¸ê¸°ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” gradient vanishing ë¬¸ì œê°€ ë°œìƒ**í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, ìƒëŒ€ì ìœ¼ë¡œ ì–•ì€ ì‹ ê²½ë§ì˜ ê°•í•œ ì„±ëŠ¥ì„ í†µí•´ ì‹ ê²½ë§ì˜ ì¤‘ê°„ layerì—ì„œ ìƒì„±ëœ íŠ¹ì§•ì´ ë§¤ìš° ì°¨ë³„ì ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œÂ **ì¤‘ê°„ layerì— auxiliary classifierë¥¼ ì¶”ê°€í•˜ì—¬, ì¤‘ê°„ì¤‘ê°„ì— ê²°ê³¼ë¥¼ ì¶œë ¥í•´ ì¶”ê°€ì ì¸ ì—­ì „íŒŒë¥¼ ì¼ìœ¼ì¼œ gradientê°€ ì „ë‹¬ë **Â ìˆ˜ ìˆê²Œë” í•˜ë©´ì„œë„Â **ì •ê·œí™” íš¨ê³¼**ê°€ ë‚˜íƒ€ë‚˜ë„ë¡ í•˜ì˜€ë‹¤.

ì¶”ê°€ë¡œ, ì§€ë‚˜ì¹˜ê²Œ ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´Â **auxiliary classifierì˜ lossì— 0.3ì„ ê³±**í•˜ì˜€ê³ ,Â **ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” auxiliary classifierë¥¼ ì œê±°**Â í›„, ì œì¼ ëë‹¨ì˜ softmaxë§Œì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

**Part 4**

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%2013.png)

Part 4ëŠ”Â **ì˜ˆì¸¡ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ëª¨ë¸ì˜ ë ë¶€ë¶„**ì´ë‹¤.

ì—¬ê¸°ì„œÂ **ìµœì¢… Classifier ì´ì „ì— average pooling layerë¥¼ ì‚¬ìš©**í•˜ê³  ìˆëŠ”ë° ì´ëŠ”Â **GAP**Â (Global Average Pooling)ê°€ ì ìš©ëœ ê²ƒìœ¼ë¡œÂ **ì´ì „ layerì—ì„œ ì¶”ì¶œëœ feature mapì„ ê°ê° í‰ê·  ë‚¸ ê²ƒì„ ì´ì–´ 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.**Â ì´ëŠ” 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ ìµœì¢…ì ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•œ softmax layerì™€ ì—°ê²°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

![img1.daumcdn.png](GoogleNet%20Summary%20(Korean)%20e82ae25140034748b6356e972f7ce196/img1.daumcdn%2014.png)

ì´ë ‡ê²ŒÂ **í‰ê· í•˜ì—¬ 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“¤ë©´ ê°€ì¤‘ì¹˜ì˜ ê°œìˆ˜ë¥¼ ìƒë‹¹íˆ ë§ì´ ì¤„ì—¬ì£¼ë©°,**Â ë˜í•œ GAPë¥¼ ì ìš©í•  ì‹œ,Â **tuning**ì„ í•˜ê¸° ì‰½ê²Œ ë§Œë“ ë‹¤.

**7. Training Methodology**

ì—¬ê¸°ì„œëŠ” ëª¨ë¸ í›ˆë ¨ì„ ì–´ë–»ê²Œ í•˜ì˜€ëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆë‹¤.

Google íŒ€ì—ì„œëŠ”Â **0.9 momentumì˜ Stochastic gradient descent**ë¥¼ ì´ìš©í•˜ì˜€ê³ ,Â **learning rateëŠ” 8 epochs ë§ˆë‹¤ 4%ì”© ê°ì†Œ**ì‹œì¼°ë‹¤.

> Momentum
ê°€ì†ë„ë¥¼ ì´ìš©í•œ  weight optimization ë°©ë²•
[https://light-tree.tistory.com/140](https://light-tree.tistory.com/140)
> 

ë˜í•œ, ì´ë¯¸ì§€ì˜Â **ê°€ë¡œ, ì„¸ë¡œ ë¹„ìœ¨ì„ 3 : 4ì™€ 4 : 3 ì‚¬ì´ë¡œ ìœ ì§€í•˜ë©° ë³¸ë˜ ì‚¬ì´ì¦ˆì˜ 8% ~ 100%ê°€ í¬í•¨ë˜ë„ë¡ ë‹¤ì–‘í•œ í¬ê¸°ì˜ patchë¥¼ ì‚¬ìš©**í•˜ì˜€ë‹¤. ê·¸ë¦¬ê³ Â **photometric distortions**ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ëŠ˜ë ¸ë‹¤ê³  í•œë‹¤.

**8. Conclusions**

**Inception êµ¬ì¡°ëŠ” Sparse êµ¬ì¡°ë¥¼ Dense êµ¬ì¡°ë¡œ ê·¼ì‚¬í™”í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ **í•˜ì˜€ë‹¤. ì´ëŠ” ê¸°ì¡´ì— CNN ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ ë°©ë²•ê³¼ëŠ” ë‹¤ë¥¸Â **ìƒˆë¡œìš´ ë°©ë²•**ì´ì—ˆìœ¼ë©°,Â **ì„±ëŠ¥ì€ ëŒ€í­ ìƒìŠ¹í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì€ ì•½ê°„ë§Œ ì¦ê°€**í•œë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

- 3ì¤„ìš”ì•½

ëª¨ë°”ì¼ê³¼ ì„ë² ë””ë“œ ìƒì—ì„œ ì˜ ì‘ë™í•˜ê¸° ìœ„í•´Â **ì»´í“¨íŒ… ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•´ì•¼ í•œë‹¤ëŠ” ìš”êµ¬**ê°€ ë†’ì•„ì¡Œë‹¤.

ì°¨ì› ì¶•ì†Œë¥¼ í†µí•œÂ **ê³„ì‚°ì–‘ ê°ì†Œ**ì™€Â **ë¹„ì„ í˜•ì„± ì¶”ê°€**Â ë‘ ê°€ì§€ë¥¼ ëª©ì ìœ¼ë¡œÂ **ì¸ì…‰ì…˜ ëª¨ë“ˆì„ ë„ì…**í–ˆë‹¤.

ì¸ì…‰ì…˜ ëª¨ë“ˆì„ í†µí•´Â **ì»´í“¨íŒ… ë¹„ìš©ì€ ì ê²Œ ìƒìŠ¹**í•˜ì§€ë§Œ,Â **ë” ê¹Šê³  ë„“ìœ¼ë©´ì„œ ì„±ëŠ¥ë„ ì¢‹ì€ GoogLeNetì„ êµ¬ì¶•**í–ˆë‹¤.

# + Code

GoogleNetì´ ìš°ìŠ¹í–ˆë˜ ì±Œë¦°ì§€ì¸ ILSVRC 2014 Classification Challenge ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨í˜•ì„ í•™ìŠµí•˜ê³  ì‹¶ì—ˆì§€ë§Œ, ì•„ë¬´ë˜ë„ ê³µì‹ì ì¸ ì±Œë¦°ì§€ ë°ì´í„°ì´ë‹¤ë³´ë‹ˆ ë‹¤ìš´ì„ ë°›ì„ ìˆ˜ ì—†ì—ˆë‹¤. ë˜í•œ 150,000ê°œì˜ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³ , 1,000ì¢…ë¥˜ì˜ ë¼ë²¨ì´ ìˆì–´ ë§¤ìš° ìš©ëŸ‰ì´ í¬ê¸° ë•Œë¬¸ì— Memory Errorê°€ ìš°ë ¤ë˜ì—ˆë‹¤.

ë”°ë¼ì„œ torchì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ì…‹ì¸ **STL-10 ë°ì´í„° ì…‹**ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. STL-10ì€ 10ê°œì˜ ë¼ë²¨ì„ ê°€ì§€ë©° ë¼ë²¨ ìˆ«ìì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. trainë°ì´í„°ì…‹ì€ 5,000ê°œ, validation ë°ì´í„°ì…‹ì€ 8,000ê°œì´ë‹¤.

- Label 0: ë¹„í–‰ê¸° âœˆï¸
- Label 1: ìƒˆ ğŸ¦
- Label 2: ì°¨ (car) ğŸš—
- Label 3: ê³ ì–‘ì´ ğŸ±
- Label 4: ì‚¬ìŠ´ ğŸ¦Œ
- Label 5: ê°œ ğŸ¶
- Label 6: ë§ ğŸ´
- Label 7: ì›ìˆ­ì´ ğŸ™‰
- Label 8: ë°° ğŸ›³ï¸
- Label 9: íŠ¸ëŸ­ ğŸšš
    
    
    Ex) ì‹¤ì œ ë°ì´í„° ë‚´ ì´ë¯¸ì§€
    

![https://storage.googleapis.com/tfds-data/visualization/fig/stl10-1.0.0.png](https://storage.googleapis.com/tfds-data/visualization/fig/stl10-1.0.0.png)

ì´í•˜ ì½”ë“œëŠ” ë‹¤ìŒì˜ ìë£Œë¥¼ ì°¸ê³ 

- [https://www.youtube.com/watch?v=uQc4Fs7yx5I&t=39s](https://www.youtube.com/watch?v=uQc4Fs7yx5I&t=39s)

ì´ì „ *Character-level cnn for text classification*ë…¼ë¬¸ ë¦¬ë·°ì—ì„œëŠ” kerasë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì˜€ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë¸ì„ ë°‘ë°”ë‹¥ë¶€í„° ë¹Œë”©í•˜ê¸° ìœ„í•´ Pytorch íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì¶•.

*GoogleNetì€ depthê°€ ê¹Šê¸°ë•Œë¬¸ì— model outputì„ í¬í•¨í•œ ì½”ë“œì˜ outputì„ ì²¨ë¶€í•˜ë©´ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ì ¸ ì²¨ë¶€í•˜ì§€ ì•ŠìŒ.

### 0) íŒ¨í‚¤ì§€ ì„¤ì¹˜

```python
# model êµ¬ì¶• ê´€ë ¨ íŒ¨í‚¤ì§€
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary
from torch import optim
from torch.optim.lr_scheduler import StepLR

# ë°ì´í„° & transformation ê´€ë ¨ íŒ¨í‚¤ì§€
from torchvision import datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os

# ì´ë¯¸ì§€ ê´€ë ¨ íŒ¨í‚¤ì§€
from torchvision import utils
import matplotlib.pyplot as plt
%matplotlib inline

# ê¸°íƒ€
import numpy as np
from torchsummary import summary
import time
import copy
```

### 1) ë°ì´í„° ì—…ë¡œë“œ

- ë¡œì»¬ ì»´í“¨í„°ì— ë‹¤ìš´ë°›ìŒ (ê²½ë¡œëŠ” path2data)

```python
path2data = '/content/drive/MyDrive/Deep Daiv/data'

# if not exists the path, make the directory
if not os.path.exists(path2data):
    os.mkdir(path2data)

# load dataset
train_ds = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())
val_ds = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())

print(len(train_ds))
print(len(val_ds))
```

- ì „ì²˜ë¦¬ - ì´ë¯¸ì§€ Transformation
    - ì‚¬ì´ì¦ˆ ë³€í™˜ : ëª¨ë“  ë°ì´í„° input size ë™ì¼í•˜ê²Œ!
    - ì •ê·œí™”
    - ì´ë¯¸ì§€ ë°˜ì „ : train ë°ì´í„°ì—ì„œë§Œ ì‹œí–‰, ë°ì´í„°ì…‹ì„ í’ë¶€í•˜ê²Œ í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ ë†’ì„
    
    ìš°ì„ , ì •ê·œí™” ë³€í™˜ì„ ìœ„í•œ í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°í•œë‹¤.
    

```python
train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_ds] # xëŠ” í”½ì…€ ê°’, _ëŠ” ë¼ë²¨, ì¦‰ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ R,G,B í‰ê· ê°’ ê³„ì‚°
train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_ds] 

train_meanR = np.mean([m[0] for m in train_meanRGB]) 
train_meanG = np.mean([m[1] for m in train_meanRGB])
train_meanB = np.mean([m[2] for m in train_meanRGB])
train_stdR = np.mean([s[0] for s in train_stdRGB])
train_stdG = np.mean([s[1] for s in trai- n_stdRGB])
train_stdB = np.mean([s[2] for s in train_stdRGB])

# validation dataset : mean, std
val_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in val_ds]
val_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in val_ds]

val_meanR = np.mean([m[0] for m in val_meanRGB])
val_meanG = np.mean([m[1] for m in val_meanRGB])
val_meanB = np.mean([m[2] for m in val_meanRGB])

val_stdR = np.mean([s[0] for s in val_stdRGB])
val_stdG = np.mean([s[1] for s in val_stdRGB])
val_stdB = np.mean([s[2] for s in val_stdRGB])

print(train_meanR, train_meanG, train_meanB)
print(val_meanR, val_meanG, val_meanB)
```

ì´ë¯¸ì§€ transformation ì‹œí–‰

```python
define the image transformation
train_transformation = transforms.Compose([
                        transforms.ToTensor(), # í…ì„œí˜•ìœ¼ë¡œ ë°”ê¾¸ê³ 
                        transforms.Resize(224), # ì‚¬ì´ì¦ˆ ì§€ì •
                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]), # ì •ê·œí™”
                        transforms.RandomHorizontalFlip(), # ì´ë¯¸ì§€ ë’¤ì§‘ê¸°, 50í¼ì„¼íŠ¸ì˜ í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ -> ë°ì´í„°ì…‹ í’ë¶€í•˜ê²Œ í•´ í•™ìŠµ í–¥ìƒ ë†’ì„
])

val_transformation = transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Resize(224),
                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]),
])
```

```python
# apply transforamtion
train_ds.transform = train_transformation
val_ds.transform = val_transformation

# create DataLoader
train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)
```

- ìƒ˜í”Œ ì´ë¯¸ì§€ í™•ì¸

```python
def show(img, y=None, color=True):
    npimg = img.numpy()
    npimg_tr = np.transpose(npimg, (1, 2, 0))
    plt.imshow(npimg_tr)

    if y is not None:
        plt.title('labels: ' + str(y))

np.random.seed(0)
torch.manual_seed(0)

grid_size=4
rnd_inds=np.random.randint(0,len(train_ds),grid_size)
print("image indices:",rnd_inds)

x_grid=[train_ds[i][0] for i in rnd_inds]
y_grid=[train_ds[i][1] for i in rnd_inds]

x_grid=utils.make_grid(x_grid, nrow=4, padding=2)
print(x_grid.shape)

# call helper function
plt.figure(figsize=(10,10))
show(x_grid,y_grid)
```

### 2) ëª¨ë¸ êµ¬ì¶•

```python
class GoogLeNet(nn.Module):
    def __init__(self,aux_logits=True, num_classes=10, init_weights=True):
        super(GoogLeNet, self).__init__()
        assert aux_logits == True or aux_logits == False
        self.aux_logits = aux_logits

        # conv_block takes in_channels, out_channels, kernel_size, stride, padding
        # Inception block takes out1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool

        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, 2, 1)
        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, 2, 1)
        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, 2, 1)
        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)

        # auxiliary classifier

        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)

        # auxiliary classifier

        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(3, 2, 1)
        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)

        self.avgpool = nn.AvgPool2d(7, 1)
        self.dropout = nn.Dropout(p=0.4)
        self.fc1 = nn.Linear(1024, num_classes)

        if self.aux_logits:
            self.aux1 = InceptionAux(512, num_classes)
            self.aux2 = InceptionAux(528, num_classes)
        else:
            self.aux1 = self.aux2 = None

        # weight initialization
        if init_weights:
            self._initialize_weights()

    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.inception3a(x)
        x = self.inception3b(x)
        x = self.maxpool3(x)
        x = self.inception4a(x)

        if self.aux_logits and self.training:
            aux1 = self.aux1(x)

        x = self.inception4b(x)
        x = self.inception4c(x)
        x = self.inception4d(x)

        if self.aux_logits and self.training:
            aux2 = self.aux2(x)

        x = self.inception4e(x)
        x = self.maxpool4(x)
        x = self.inception5a(x)
        x = self.inception5b(x)
        x = self.avgpool(x)

        x = x.view(x.shape[0], -1)

        x = self.dropout(x)
        x = self.fc1(x)

        if self.aux_logits and self.training:
            return x, aux1, aux2
        else:
            return x 

    # define weight initialization function
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
```

```python
class conv_block(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(conv_block, self).__init__()

        self.conv_layer = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, **kwargs),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )
    
    def forward(self, x):
        return self.conv_layer(x)
```

```python
class Inception_block(nn.Module):
    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):
        super(Inception_block, self).__init__()

        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)

        self.branch2 = nn.Sequential(
            conv_block(in_channels, red_3x3, kernel_size=1),
            conv_block(red_3x3, out_3x3, kernel_size=3, padding=1),
        )

        self.branch3 = nn.Sequential(
            conv_block(in_channels, red_5x5, kernel_size=1),
            conv_block(red_5x5, out_5x5, kernel_size=5, padding=2),
        )

        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            conv_block(in_channels, out_1x1pool, kernel_size=1)
        )

    def forward(self, x):
        # 0ì°¨ì›ì€ batchì´ë¯€ë¡œ 1ì°¨ì›ì¸ filter ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° branchì˜ ì¶œë ¥ê°’ì„ ë¬¶ì–´ì¤ë‹ˆë‹¤. 
        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)
        return x
```

```python
# auxiliary classifierì˜ lossëŠ” 0.3ì´ ê³±í•´ì§€ê³ , ìµœì¢… lossì— ì¶”ê°€í•©ë‹ˆë‹¤. ì •ê·œí™” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. 
class InceptionAux(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(InceptionAux, self).__init__()

        self.conv = nn.Sequential(
            nn.AvgPool2d(kernel_size=5, stride=3),
            conv_block(in_channels, 128, kernel_size=1),
        )

        self.fc = nn.Sequential(
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(1024, num_classes),
        )

    def forward(self,x):
        x = self.conv(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)
        return x
```

```python
# GPU ì‚¬ìš©
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # GPU ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´ í•´!
print(device)

model = GoogLeNet(aux_logits=True, num_classes=10, init_weights=True).to(device)
print(model)
```

```python
summary(model, input_size=(3,224,224), device=device.type)
```

### 3) í•™ìŠµ

```python
loss_func = nn.CrossEntropyLoss(reduction='sum')
opt = optim.Adam(model.parameters(), lr=0.001)

from torch.optim.lr_scheduler import StepLR
lr_scheduler = StepLR(opt, step_size=30, gamma=0.1)
```

```python
def get_lr(opt):
    for param_group in opt.param_groups:
        return param_group['lr']

def metric_batch(output, target):
    pred = output.argmax(dim=1, keepdim=True)
    corrects = pred.eq(target.view_as(pred)).sum().item()
    return correct
```

```python
def loss_batch(loss_func, outputs, target, opt=None):
    if np.shape(outputs)[0] == 3:
        output, aux1, aux2 = outputs

        output_loss = loss_func(output, target)
        aux1_loss = loss_func(aux1, target)
        aux2_loss = loss_func(aux2, target)

        loss = output_loss + 0.3*(aux1_loss + aux2_loss)
        metric_b = metric_batch(output,target)

    else:
        loss = loss_func(outputs, target)
        metric_b = metric_batch(outputs, target)

    if opt is not None:
        opt.zero_grad()
        loss.backward()
        opt.step()
    
    return loss.item(), metric_b
```

```python
def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):
    running_loss = 0.0
    running_metric = 0.0
    len_data = len(dataset_dl.dataset)

    for xb, yb in dataset_dl:
        xb = xb.to(device)
        yb = yb.to(device)
        output= model(xb)

        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)

        running_loss += loss_b

        if metric_b is not None:
            running_metric += metric_b
        
        if sanity_check is True:
            break

    loss = running_loss / len_data
    metric = running_metric / len_data

    return loss, metric

def train_val(model, params):
    num_epochs=params["num_epochs"]
    loss_func=params["loss_func"]
    opt=params["optimizer"]
    train_dl=params["train_dl"]
    val_dl=params["val_dl"]
    sanity_check=params["sanity_check"]
    lr_scheduler=params["lr_scheduler"]
    path2weights=params["path2weights"]

    loss_history = {'train': [], 'val': []}
    metric_history = {'train': [], 'val': []}

    best_model_wts = copy.deepcopy(model.state_dict())

    best_loss = float('inf')
    
    start_time = time.time()
    for epoch in range(num_epochs):
        current_lr = get_lr(opt)
        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))
        
        model.train()
        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)
        loss_history['train'].append(train_loss)
        metric_history['train'].append(train_metric)

        model.eval()
        with torch.no_grad():
            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)

        if val_loss < best_loss:
            best_loss = val_loss
            best_model_wts = copy.deepcopy(model.state_dict())

            torch.save(model.state_dict(), path2weights)
            print('Copied best model weights!')

        loss_history['val'].append(val_loss)
        metric_history['val'].append(val_metric)

        lr_scheduler.step()

        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))
        print('-'*10)

    model.load_state_dict(best_model_wts)

    return model, loss_history, metric_history
```

- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜

```python
# definc the training parameters
params_train = {
    'num_epochs':10,
    'optimizer':opt,
    'loss_func':loss_func,
    'train_dl':train_dl,
    'val_dl':val_dl,
    'sanity_check':False,
    'lr_scheduler':lr_scheduler,
    'path2weights':'./models/weights.pt',
}

# create the directory that stores weights.pt
def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSerror:
        print('Error')
createFolder('./models')
```

```python
model, loss_hist, metric_hist = train_val(model, params_train)
```

- ê²°ê³¼ ì‹œê°í™”

```python
num_epochs=params_train["num_epochs"]

# plot loss progress
plt.title("Train-Val Loss")
plt.plot(range(1,num_epochs+1),loss_hist["train"],label="train")
plt.plot(range(1,num_epochs+1),loss_hist["val"],label="val")
plt.ylabel("Loss")
plt.xlabel("Training Epochs")
plt.legend()
plt.show()

# plot accuracy progress
plt.title("Train-Val Accuracy")
plt.plot(range(1,num_epochs+1),metric_hist["train"],label="train")
plt.plot(range(1,num_epochs+1),metric_hist["val"],label="val")
plt.ylabel("Accuracy")
plt.xlabel("Training Epochs")
plt.legend()
plt.show()
```
